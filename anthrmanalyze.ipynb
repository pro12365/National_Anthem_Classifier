{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:green\"><b> National Anthem Classifier </b></h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Importing Packages needed<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt',quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import  word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Loading Dataset</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Country Alpha-2 Alpha-3 Continent  \\\n",
      "0     Albania      AL     ALB    Europe   \n",
      "1     Armenia      AM     ARM    Europe   \n",
      "2     Austria      AT     AUT    Europe   \n",
      "3  Azerbaijan      AZ     AZE    Europe   \n",
      "4     Belarus      BY     BLR    Europe   \n",
      "\n",
      "                                              Anthem  \n",
      "0  Around our flag we stand united, With one wish...  \n",
      "1  Our Fatherland, free, independent, That has fo...  \n",
      "2  Land of mountains, land by the river, Land of ...  \n",
      "3  Azerbaijan, Azerbaijan! The glorious Fatherlan...  \n",
      "4  We, Belarusians, are peaceful people, Wholehea...  \n"
     ]
    }
   ],
   "source": [
    "#loading the csv file using pandas\n",
    "data= pd.read_csv('national_anthems.csv')\n",
    "#Creating Dataframes\n",
    "df= pd.DataFrame(data)\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Data Cleaning And Data Pre-processing</b></h5>\n",
    "<p><span><b>Preprocessing the data</b><span>: The first step is to preprocess the data. The Kaggle dataset of National Anthems only contains the lyrics of national anthems in English. However, some of the lyrics may contain special characters or words that are not relevant for our classification task. Therefore, we need to remove them using techniques such as tokenization, stemming, and stopword removal.\n",
    "<span><b>preprocessing consist of 5 steps</b><br>\n",
    "<span><b>Substitution:-</b> </span>Substituting regex like speicial characters and digits with empty space<br> \n",
    "<span><b>Generalization:</b></span>Converting the texts to a generalized format here we are Generalizing text case<br>\n",
    "<span><b>Tokenization:-</b></span>Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.<br> \n",
    "<span><b>Stemming:-</b></span>Stemming is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as \"lemmas\". Stemming is important in natural language understanding (NLU) and natural language processing (NLP) for example <span style=\"color:red\">Chocolaty, chocolacious, choco, to Stemmed word Chocolate </b></span><br>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing Data\n",
    "def preprocess(text):\n",
    "    #substituting speicial characters and digits with empty string\n",
    "    text=re.sub('[^a-zA-Z]', ' ',text)\n",
    "    #converting text to lowercase\n",
    "    text= text.lower()\n",
    "    #tokenizing the text\n",
    "    words= word_tokenize(text)\n",
    "    #removing stopwords and stemming words\n",
    "    ps= PorterStemmer()\n",
    "    words= [ps.stem(word) for word in words if not word in set(stopwords.words('english'))]\n",
    "    #joining the words back into string \n",
    "    preprocessed_text=' '.join(words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      around flag stand unit one wish one goal sacr ...\n",
      "1      fatherland free independ centuri live summon s...\n",
      "2      land mountain land river land field land cathe...\n",
      "3      azerbaijan azerbaijan gloriou fatherland valor...\n",
      "4      belarusian peac peopl wholeheartedli devot mot...\n",
      "                             ...                        \n",
      "185    defend homeland ralli around glori time blood ...\n",
      "186    oh uganda may god uphold thee lay futur thi ha...\n",
      "187    son sahara battlefield torch holder long road ...\n",
      "188    stand sing zambia proud free land work joy uni...\n",
      "189    oh lift high banner flag zimbabw symbol freedo...\n",
      "Name: Anthem, Length: 190, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['Anthem']=df['Anthem'].apply(preprocess)\n",
    "print(df['Anthem'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Feature Extraction</b></h5>\n",
    "<p><b>Feature Extraction:</b>Feature extraction for machine learning and deep learning. Feature extraction refers to the process of transforming raw data <br>into numerical features that can be processed while preserving the information in the original data set.</h5>\n",
    "<h5><b>TF-IDF VECTORIZER</b></h5>\n",
    "<p>The word count from text documents is very basic at the starting point. However simple word count is not sufficient for text processing because of the words like “the”, “an”, “your”, etc. are highly occurred in text documents. Their large word count is meaningless towards the analysis of the text. Tf-idf can be successfully used for stop-words filtering from the text document.\n",
    "\n",
    "The other way to solve this problem is word frequency. This method is called the TF-IDF stands for “Term Frequency – Inverse Document Frequency ”. TF-IDF is a numerical statistic which measures the importance of the word in a document.\n",
    "\n",
    "<b>Term Frequency:</b> Number of time a word appears in a text document.<br>\n",
    "<b>Inverse Document Frequency</b>: Measure the word is a rare word or common word in a document.</p>\n",
    "<h5><b>TF-IDF VECTORIZER Formulae</b></h5>\n",
    "<img src=\"Markdown_Images/tf-idf-1.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.09736479 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.28640834 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Feature Extraction\n",
    "tfidf= TfidfVectorizer()\n",
    "features= tfidf.fit_transform(df['Anthem']).toarray()\n",
    "print(features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Splitting:</b></h5>\n",
    "<p>n machine learning, data splitting is typically done to avoid overfitting. That is an instance where a machine learning model fits its training data too well and fails to reliably fit additional data.<br>\n",
    "\n",
    "The original data in a machine learning model is typically taken and split into three or four sets. The three sets commonly used are the training set, the dev set and the testing set:<p>\n",
    "<li>The <span style=\"color:red\"><b>training set</b></span> is the portion of data used to train the model. The model should observe and learn from the training set, optimizing any of its parameters.</li>\n",
    "<li>The <span style=\"color:red\"><b>devset</b></span> is a data set of examples used to change learning process parameters. It is also called the cross-validation or model validation set. This set of data has the goal of ranking the model's accuracy and can help with model selection</li>\n",
    "<li>The <span style=\"color:red\"><b>testing set</b></span> is the portion of data that is tested in the final model and is compared against the previous sets of data. The testing set acts as an evaluation of the final mode and algorithm.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Data\n",
    "X_train,X_test,y_train,y_test=train_test_split(features,df['Country'],test_size=0.2,random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Training The Model</b></h5>\n",
    "<p>The Model Chosen Here is<b>Logistic Regression Model</b></p>\n",
    "<li><span><b>Logistic Regression Model:</b></span> is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables</li>\n",
    "<li> Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.</li>\n",
    "<li>Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.</li>\n",
    "<li>In Logistic regression, instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1).</li>\n",
    "<li>Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets..</li>\n",
    "<li>Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function:</li>\n",
    "<img src=\"Markdown_Images/logistic-regression-in-machine-learning.png\"><br>\n",
    "<h8><b>Logistic Function(Sygmoid Function):</b></h8>\n",
    "<li>The sigmoid function is a mathematical function used to map the predicted values to probabilities.</li>\n",
    "<li>It maps any real value into another value within a range of 0 and 1.</li>\n",
    "<li>The value of the logistic regression must be between 0 and 1, which cannot go beyond this limit, so it forms a curve like the \"S\" form. The S-form curve is called the Sigmoid function or the logistic function</li>\n",
    "<li>In logistic regression, we use the concept of the threshold value, which defines the probability of either 0 or 1. Such as values above the threshold value tends to 1, and a value below the threshold values tends to 0</li>\n",
    "<h8><b>Equations Logistic Regression:</b></h8>\n",
    "<li>The Logistic regression equation can be obtained from the Linear Regression equation. The mathematical steps to get Logistic Regression equations are given below:</li>\n",
    "<img src=\"Markdown_Images/logistic-regression-in-machine-learning2.png\">\n",
    "<li>In Logistic Regression y can be between 0 and 1 only, so for this let's divide the above equation by (1-y):</li>\n",
    "<img src=\"Markdown_Images/logistic-regression-in-machine-learning3.png\">\n",
    "<li>But we need range between -[infinity] to +[infinity], then take logarithm of the equation it will become:</li>\n",
    "<img src=\"Markdown_Images/logistic-regression-in-machine-learning4.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training The Model\n",
    "model= LogisticRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Evaluating The Models</b><h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.0\n",
      "Prediction: ['Sweden']\n"
     ]
    }
   ],
   "source": [
    "y_pred= model.predict(X_test)\n",
    "score=accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy\",score)\n",
    "# Predicting new data\n",
    "new_data = ['You ancient, you free, you mountainous north You quiet, you joyfully beauty! I greet you, loveliest land upon Earth, Your sun, your sky, your green climes. You throne on memories of great olden days, When honoured your name flew across the Earth, I know that you are and will become what you were, Yes, I want to live, I want to die in the North. I always will serve you my beloved land, your fidelity to death I will swear. Your right, I shall defend, with mind and with hand, your banner, high with feats it carries. With God I shall fight, for home and for hearth, for Sweden, the beloved homeland Exchange you, I wont for anything in this world No, I want live, I want to die in the North']\n",
    "new_data = tfidf.transform(new_data).toarray()\n",
    "prediction = model.predict(new_data)\n",
    "print('Prediction:', prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
